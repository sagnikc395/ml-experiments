{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87590e34-fb66-4db9-9e78-04cc98317d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# following along this blog post to gain more understanding about RNNs\n",
    "# https://jaketae.github.io/study/pytorch-rnn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09494e87-bcb3-46ca-bc37-059d070db778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task -> Build a simple classification model that can correctly determine the \n",
    "# nationality of a person given their name. Simple , we want to be able to tell where a \n",
    "# particular name is from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05ce84ec-6736-403c-821d-3266d1275e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2814k  100 2814k    0     0  1763k      0  0:00:01  0:00:01 --:--:-- 1762k\n"
     ]
    }
   ],
   "source": [
    "# download and unzip data in current directory\n",
    "!curl -O https://download.pytorch.org/tutorial/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a15920dd-252c-4531-a3df-58acb7fb9c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "replace data/eng-fra.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12d1c0d7-5182-4bbf-8b79-f7c4cbef4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the data in more detail\n",
    "import os\n",
    "import random \n",
    "\n",
    "# setup pytorch \n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from unidecode import unidecode\n",
    "\n",
    "seed_value = torch.manual_seed(42)\n",
    "# change the device to GPU if its there else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "433da8f1-96eb-43c6-ae5f-858da21c54fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifying a directory and try to print all the labels that are there in the directory\n",
    "data_directory = \"./data/names\"\n",
    "\n",
    "# then construct a dictionary that will map a language to a numerical label\n",
    "lang2label = {}\n",
    "for i, file_name in enumerate(os.listdir(data_directory)):\n",
    "    key = file_name.split(\".\")[0]\n",
    "    val = torch.tensor([i],dtype=torch.long)\n",
    "    lang2label[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d670ea7-c539-4d9f-823d-710f2d3a78e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Czech': tensor([0]), 'German': tensor([1]), 'Arabic': tensor([2]), 'Japanese': tensor([3]), 'Chinese': tensor([4]), 'Vietnamese': tensor([5]), 'Russian': tensor([6]), 'French': tensor([7]), 'Irish': tensor([8]), 'English': tensor([9]), 'Spanish': tensor([10]), 'Greek': tensor([11]), 'Italian': tensor([12]), 'Portuguese': tensor([13]), 'Scottish': tensor([14]), 'Dutch': tensor([15]), 'Korean': tensor([16]), 'Polish': tensor([17])}\n"
     ]
    }
   ],
   "source": [
    "#checking what lang2label contains \n",
    "print(lang2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9df45195-921d-4d41-a09a-8dd5d7964bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of languages\n",
    "num_langs = len(lang2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27854640-03a0-4c8f-baa4-9ae93244862b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1955e834-93f4-46be-b1ff-bf8a8b343dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Slusarski'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing Stage \n",
    "# preprocessing the names -> first want to use unidecode to standardize all the names and remove \n",
    "# any acute symbols or the likes\n",
    "\n",
    "unidecode(\"Ślusàrski\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd4feca4-d14e-4ee0-bda5-b29477cc88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting a decoded string to a tensor so that the model can process it \n",
    "from string import ascii_letters\n",
    "char2idx = {letter: i for i , letter in enumerate(ascii_letters + \" .,:;-'\")}\n",
    "num_letters = len(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae699100-62b1-4b7a-92f3-90bb60d012b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the total number of tokens in our character vocabulary \n",
    "num_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83e8d14c-48ee-4981-8d5a-8b34fffec23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a function that accomplishes the task\n",
    "def name2tensor(name):\n",
    "    tensor = torch.zeros(len(name),1,num_letters)\n",
    "    for i, char in enumerate(name):\n",
    "        tensor[i][0][char2idx[char]] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d794e759-3d73-4a19-9f9b-d7e7dfb9b12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name2tensor(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4738e1a-91ab-4a8d-8b0b-5653a87058d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Creation ->\n",
    "# Need to build our dataset with all the preprocessing steps. \n",
    "# collecting all the decoded and converted tensors in a list, with accompanying labels.\n",
    "# the labels can be obtained easily from the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94515535-4eba-4852-8ea4-b3ad9c3d8e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_names = []\n",
    "target_langs = []\n",
    "\n",
    "for file in os.listdir(data_directory):\n",
    "    with open(os.path.join(data_directory,file)) as f:\n",
    "        lang = file.split(\".\")[0]\n",
    "        names = [unidecode(line.rstrip()) for line in f]\n",
    "        for name in names:\n",
    "            try:\n",
    "                tensor_names.append(name2tensor(name))\n",
    "                target_langs.append(lang2label[lang])\n",
    "            except KeyError:\n",
    "                pass\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39af102c-6ebb-4d61-924c-5283968b032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sklearn's train_test_split() to seperate the training data from the testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4325e8cd-ca46-48eb-8e52-6914bd012783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9785524-b41e-47b1-b3bc-09617f138683",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, test_idx = train_test_split(\n",
    "    range(len(target_langs)),\n",
    "    test_size = 0.1,\n",
    "    shuffle=True,\n",
    "    stratify=target_langs\n",
    ")\n",
    "# making the training dataset \n",
    "train_dataset = []\n",
    "for i in train_idx:\n",
    "    train_dataset.append((tensor_names[i],target_langs[i]))\n",
    "\n",
    "# making the test dataset \n",
    "test_dataset = []\n",
    "for i in test_idx:\n",
    "    test_dataset.append((tensor_names[i],target_langs[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "414bf3c6-2681-4df2-93f5-2a79012e299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset size: 18063\n",
      "Test Dataset size: 2007\n"
     ]
    }
   ],
   "source": [
    "# printing our train and test dataset\n",
    "print(f\"Train Dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test Dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "78b2385d-f531-4613-9fcf-8047f68d3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Simple RNN model\n",
    "# A simple RNN that takes a single character tensor repr as input and produces \n",
    "# some prediction and a hidden state, which can be used in the next iteration.\n",
    "# just some fully connected layers with sigmoid non-linearity applied during the hidden \n",
    "# state computation \n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(SimpleRNN,self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in2hidden = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.in2output = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self,x,hidden_state):\n",
    "        combined = torch.cat((x,hidden_state),1)\n",
    "        hidden = torch.sigmoid(self.in2hidden(combined))\n",
    "        output = self.in2output(combined)\n",
    "        return output, hidden \n",
    "\n",
    "    def init_hidden(self):\n",
    "        return nn.init.kaiming_uniform_(torch.empty(1,self.hidden_size))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8521efd9-2bd6-4b69-9d47-e2dd4d9db19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling init_hidden() at the start of every new batch. For easier training and learning \n",
    "# using kaiming_uniform_() to initialize these hidden states\n",
    "\n",
    "# building our model and training it \n",
    "hidden_size = 256\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = SimpleRNN(num_letters,hidden_size,num_langs)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b7f96d5-64d2-4d43-b085-172ecf122f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [3000/18063], Loss: 0.0000\n",
      "Epoch [1/10], Step [6000/18063], Loss: 0.0077\n",
      "Epoch [1/10], Step [9000/18063], Loss: 0.5191\n",
      "Epoch [1/10], Step [12000/18063], Loss: 0.0159\n",
      "Epoch [1/10], Step [15000/18063], Loss: 0.0025\n",
      "Epoch [1/10], Step [18000/18063], Loss: 0.0012\n",
      "Epoch [2/10], Step [3000/18063], Loss: 0.1433\n",
      "Epoch [2/10], Step [6000/18063], Loss: 0.0010\n",
      "Epoch [2/10], Step [9000/18063], Loss: 1.6404\n",
      "Epoch [2/10], Step [12000/18063], Loss: 2.3458\n",
      "Epoch [2/10], Step [15000/18063], Loss: 0.0060\n",
      "Epoch [2/10], Step [18000/18063], Loss: 0.0076\n",
      "Epoch [3/10], Step [3000/18063], Loss: 0.0395\n",
      "Epoch [3/10], Step [6000/18063], Loss: 2.5282\n",
      "Epoch [3/10], Step [9000/18063], Loss: 0.0309\n",
      "Epoch [3/10], Step [12000/18063], Loss: 0.0148\n",
      "Epoch [3/10], Step [15000/18063], Loss: 0.0016\n",
      "Epoch [3/10], Step [18000/18063], Loss: 0.0000\n",
      "Epoch [4/10], Step [3000/18063], Loss: 0.0245\n",
      "Epoch [4/10], Step [6000/18063], Loss: 1.0341\n",
      "Epoch [4/10], Step [9000/18063], Loss: 0.2133\n",
      "Epoch [4/10], Step [12000/18063], Loss: 0.0095\n",
      "Epoch [4/10], Step [15000/18063], Loss: 0.0028\n",
      "Epoch [4/10], Step [18000/18063], Loss: 0.0054\n",
      "Epoch [5/10], Step [3000/18063], Loss: 0.0000\n",
      "Epoch [5/10], Step [6000/18063], Loss: 0.0004\n",
      "Epoch [5/10], Step [9000/18063], Loss: 0.0047\n",
      "Epoch [5/10], Step [12000/18063], Loss: 0.0083\n",
      "Epoch [5/10], Step [15000/18063], Loss: 0.0000\n",
      "Epoch [5/10], Step [18000/18063], Loss: 1.2114\n",
      "Epoch [6/10], Step [3000/18063], Loss: 0.1192\n",
      "Epoch [6/10], Step [6000/18063], Loss: 0.0226\n",
      "Epoch [6/10], Step [9000/18063], Loss: 0.2587\n",
      "Epoch [6/10], Step [12000/18063], Loss: 0.0000\n",
      "Epoch [6/10], Step [15000/18063], Loss: 0.0024\n",
      "Epoch [6/10], Step [18000/18063], Loss: 0.5171\n",
      "Epoch [7/10], Step [3000/18063], Loss: 0.0000\n",
      "Epoch [7/10], Step [6000/18063], Loss: 0.0237\n",
      "Epoch [7/10], Step [9000/18063], Loss: 0.4957\n",
      "Epoch [7/10], Step [12000/18063], Loss: 0.9846\n",
      "Epoch [7/10], Step [15000/18063], Loss: 0.0004\n",
      "Epoch [7/10], Step [18000/18063], Loss: 0.0000\n",
      "Epoch [8/10], Step [3000/18063], Loss: 0.0008\n",
      "Epoch [8/10], Step [6000/18063], Loss: 0.0003\n",
      "Epoch [8/10], Step [9000/18063], Loss: 0.0360\n",
      "Epoch [8/10], Step [12000/18063], Loss: 0.0093\n",
      "Epoch [8/10], Step [15000/18063], Loss: 0.0002\n",
      "Epoch [8/10], Step [18000/18063], Loss: 0.0000\n",
      "Epoch [9/10], Step [3000/18063], Loss: 0.0000\n",
      "Epoch [9/10], Step [6000/18063], Loss: 0.3617\n",
      "Epoch [9/10], Step [9000/18063], Loss: 0.1038\n",
      "Epoch [9/10], Step [12000/18063], Loss: 0.0000\n",
      "Epoch [9/10], Step [15000/18063], Loss: 0.2082\n",
      "Epoch [9/10], Step [18000/18063], Loss: 3.2162\n",
      "Epoch [10/10], Step [3000/18063], Loss: 1.6009\n",
      "Epoch [10/10], Step [6000/18063], Loss: 0.0004\n",
      "Epoch [10/10], Step [9000/18063], Loss: 0.0553\n",
      "Epoch [10/10], Step [12000/18063], Loss: 0.0074\n",
      "Epoch [10/10], Step [15000/18063], Loss: 0.0000\n",
      "Epoch [10/10], Step [18000/18063], Loss: 4.3384\n"
     ]
    }
   ],
   "source": [
    "## training our model\n",
    "num_epochs = 10\n",
    "print_interval = 3000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    random.shuffle(train_dataset)\n",
    "    for i, (name,label) in enumerate(train_dataset):\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name:\n",
    "            output, hidden_state = model(char,hidden_state)\n",
    "        loss = criterion(output,label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),1)\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % print_interval == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "                f\"Step [{i + 1}/{len(train_dataset)}], \"\n",
    "                f\"Loss: {loss.item():.4f}\"\n",
    "            )            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a5d4a0c-f580-4b9c-bfa3-b7bb77d4c511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 82.6109%\n"
     ]
    }
   ],
   "source": [
    "# testing our model , looking at accuracy factor \n",
    "num_correct = 0 \n",
    "num_samples = len(test_dataset)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name,label in test_dataset:\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in name:\n",
    "            output,hidden_state = model(char,hidden_state)\n",
    "        _,pred = torch.max(output,dim=1)\n",
    "        num_correct += bool(pred == label)\n",
    "\n",
    "print(f\"Accuracy: {num_correct/ num_samples * 100:.4f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f524b4f-f672-4431-aba6-55df66a99cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking our model with some concrete examples \n",
    "label2lang = {}\n",
    "for lang, label in lang2label.items():\n",
    "    label2lang[label.item()] = lang \n",
    "\n",
    "def myrnn_predict(name):\n",
    "    model.eval()\n",
    "    tensor_name = name2tensor(name)\n",
    "    with torch.no_grad():\n",
    "        hidden_state = model.init_hidden()\n",
    "        for char in tensor_name:\n",
    "            output, hidden_state = model(char,hidden_state)\n",
    "        _,pred = torch.max(output,dim=1)\n",
    "    model.train()\n",
    "    return label2lang[pred.item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4bf673c0-b2f7-4bdc-94ba-12969fa79b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Japanese'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrnn_predict(\"Mike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42674280-b7eb-44bf-b880-5360bdd9622f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chinese'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrnn_predict(\"Qin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d12e8c61-2d72-4e06-94a3-856c4ddb223a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arabic'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrnn_predict(\"Sagnik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ba59e-53fc-43e7-86a1-50c3b72f0e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
